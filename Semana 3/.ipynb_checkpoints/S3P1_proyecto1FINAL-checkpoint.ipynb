{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2023/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 1 - Predicción de precios de vehículos usados\n",
    "\n",
    "En este proyecto podrán poner en práctica sus conocimientos sobre modelos predictivos basados en árboles y ensambles, y sobre la disponibilización de modelos. Para su desasrrollo tengan en cuenta las instrucciones dadas en la \"Guía del proyecto 1: Predicción de precios de vehículos usados\".\n",
    "\n",
    "**Entrega**: La entrega del proyecto deberán realizarla durante la semana 4. Sin embargo, es importante que avancen en la semana 3 en el modelado del problema y en parte del informe, tal y como se les indicó en la guía.\n",
    "\n",
    "Para hacer la entrega, deberán adjuntar el informe autocontenido en PDF a la actividad de entrega del proyecto que encontrarán en la semana 4, y subir el archivo de predicciones a la [competencia de Kaggle](https://www.kaggle.com/t/b8be43cf89c540bfaf3831f2c8506614)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos para la predicción de precios de vehículos usados\n",
    "\n",
    "En este proyecto se usará el conjunto de datos de Car Listings de Kaggle, donde cada observación representa el precio de un automóvil teniendo en cuenta distintas variables como: año, marca, modelo, entre otras. El objetivo es predecir el precio del automóvil. Para más detalles puede visitar el siguiente enlace: [datos](https://www.kaggle.com/jpayne/852k-used-car-listings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación librerías\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randrange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos de archivo .csv\n",
    "dataTraining = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2023/main/datasets/dataTrain_carListings.zip')\n",
    "dataTesting = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2023/main/datasets/dataTest_carListings.zip', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualización datos de entrenamiento\n",
    "dataTraining.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining['Make'] = dataTraining['Make'].astype('category')\n",
    "dataTraining['Model'] = dataTraining['Model'].astype('category')\n",
    "dataTraining['State'] = dataTraining['State'].astype('category')\n",
    "dataTraining.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de datos antes del preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_make = dataTraining['Make'].value_counts()\n",
    "conteo_make.plot.bar(figsize=(10,6), fontsize=12, color='blue')\n",
    "plt.title('Cantidad de marcas de autos', fontsize=16)\n",
    "plt.xlabel('Marca', fontsize=14)\n",
    "plt.ylabel('Cantidad', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MatrixCorrelacion = dataTraining.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.set(font_scale=1.5)  \n",
    "\n",
    "sns.heatmap(MatrixCorrelacion, annot=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma para todas las variables numéricas incluyendo la variable de respuesta\n",
    "dataTraining.hist(bins=50, figsize=(20,15))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dataTraining.Year,dataTraining.Price)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dataTraining.Mileage,dataTraining.Price)\n",
    "plt.xlabel(\"Mileage\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos por Estado y calculamos el promedio de los precios\n",
    "data = dataTraining.groupby('State')['Price'].mean()\n",
    "\n",
    "# Ordenamos los valores de mayor a menor\n",
    "data = data.sort_values(ascending=False)\n",
    "\n",
    "# Creamos la gráfica de barras\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(data.index, data.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Precio Promedio')\n",
    "plt.title('Promedio de precios por estado')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edad del vehículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = datetime.datetime.now()\n",
    "dataTraining['Age'] = dataTraining['Year'].apply(lambda x : time_now.year - x)\n",
    "dataTraining = dataTraining.drop(['Year'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminación de duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados\n",
    "duplicados = dataTraining.duplicated()\n",
    "print(\"Número total de duplicados en el DataFrame: \", duplicados.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining = dataTraining.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminación de valores atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALORES ATÍPICOS PARA MILEAGE\n",
    "# Calcular el rango intercuartil (IQR)\n",
    "Q1 = dataTraining['Mileage'].quantile(0.25)\n",
    "Q3 = dataTraining['Mileage'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Eliminar los valores atípicos utilizando el método del rango intercuartil (IQR)\n",
    "dataTraining = dataTraining[~((dataTraining['Mileage'] < (Q1 - 1.5 * IQR)) | (dataTraining['Mileage'] > (Q3 + 1.5 * IQR)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(dataTraining['Mileage'], vert=False)\n",
    "plt.title('Boxplot de la variable \"mileage\"', fontsize=16)\n",
    "plt.xlabel('Mileage', fontsize=14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estandarización Mileage en data Training y Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit transform\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataTraining[['Mileage']])\n",
    "dataTraining[['Mileage']] = scaler.transform(dataTraining[['Mileage']])\n",
    "dataTesting[['Mileage']] = scaler.transform(dataTesting[['Mileage']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Marca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar espacios en la columna \"Make\"\n",
    "dataTraining[\"Make\"] = dataTraining[\"Make\"].apply(lambda x: x.strip())\n",
    "\n",
    "############################## Dummies para Marca ####################################\n",
    "dummies = pd.get_dummies(dataTraining['Make'], prefix='Make')\n",
    "dummies=dummies.drop('Make_Freightliner', axis=1)\n",
    "dataTraining = pd.concat([dataTraining, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelo\n",
    "En el caso de la variable modelo, se crea una variable dummie teniendo en cuenta la media del precio para un modelo específico. Después se mapean esas particiones para asignar la misma dummie en la data de testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Dummies para los modelos ##########################\n",
    "\n",
    "# Crear un diccionario vacío para almacenar los resultados\n",
    "promedios_por_modelo = {}\n",
    "\n",
    "# Recorrer los valores distintos de la columna \"Model\"\n",
    "for model in dataTraining[\"Model\"].unique():\n",
    "    # Obtener el promedio de la columna \"Price\" para los registros donde \"Model\" es igual a la marca actual\n",
    "    promedio = dataTraining.loc[dataTraining[\"Model\"] == model, \"Price\"].mean()\n",
    "    # Agregar la marca y su promedio al diccionario\n",
    "    promedios_por_modelo[model] = promedio\n",
    "diccionario_ordenado = dict(sorted(promedios_por_modelo.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "column_names = []\n",
    "for i in range(0, 80000, int(80000/k)):\n",
    "    name = f\"{i}-{i+int(80000/k)}\"\n",
    "    column_names.append(name)\n",
    "\n",
    "# Creamos las columnas con el nombre de la partición en el dataframe\n",
    "for i in range(len(column_names)):\n",
    "    dataTraining[column_names[i]]=0\n",
    "\n",
    "# Creamos diccionario con el rango de precios como llaves y los modelos que se encuentran en ese rango como valores\n",
    "rango_precios = {}\n",
    "\n",
    "for column in column_names:\n",
    "    start, end = column.split(\"-\")\n",
    "    start, end = int(start), int(end)\n",
    "    rango_precios[column] = [key for key, value in diccionario_ordenado.items() if start <= value <= end]\n",
    "\n",
    "# Validar si el valor de la columna Model está en el diccionario rango_precios\n",
    "for i, row in dataTraining.iterrows():\n",
    "    model = row['Model']\n",
    "    for key, value in rango_precios.items():\n",
    "        if model in value:\n",
    "            dataTraining.at[i, key] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar espacios en la columna \"State\"\n",
    "dataTraining[\"State\"] = dataTraining[\"State\"].apply(lambda x: x.strip())\n",
    "\n",
    "############################## Dummies para State ####################################\n",
    "dummies = pd.get_dummies(dataTraining['State'], prefix='State')\n",
    "dataTraining = pd.concat([dataTraining, dummies], axis=1)\n",
    "\n",
    "dataTraining = dataTraining.drop(['State', 'Make', 'Model'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación variable \"Price\"\n",
    "En la gráfica a continuación, se puede observar que la distribución de la variable \"Price\" es asimétrica hacia la derecha. Por lo tanto, para la construcción del modelo se transforma la variable de respuesta con el logaritmo y después se aprecia que la distribución de la variable es casi normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "fig, ax = plt.subplots(1,2)\n",
    "width, height = fig.get_size_inches()\n",
    "fig.set_size_inches(width*2, height)\n",
    "sns.distplot(dataTraining['Price'], ax=ax[0], fit=norm)\n",
    "sns.distplot(np.log(dataTraining[('Price')]+1), ax=ax[1], fit= norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining['Price'] = np.log1p(dataTraining['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separación de datos en set de entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de variables predictoras (X) y variable de interés (y)\n",
    "y = dataTraining['Price']\n",
    "X = dataTraining.drop(['Price'], axis=1)\n",
    "\n",
    "# Separación de datos en set de entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibración del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEstimators = 350\n",
    "nSubsample = 0.8\n",
    "nDepth = 10\n",
    "nLearningRate = 0.3\n",
    "nGamma = 0\n",
    "nColsample = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELO = XGBRegressor(n_estimators=nEstimators, subsample=nSubsample, max_depth=nDepth, learning_rate=nLearningRate,\n",
    "                        gamma=nGamma, colsample_bytree=nColsample, random_state=12345, n_jobs=-1)\n",
    "MODELO.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impresión de desempeño del modelo\n",
    "y_pred = MODELO.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test = np.expm1(y_test)\n",
    "y_pred = np.expm1(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "xgb_RMSE = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"RMSE: %.3f\" %xgb_RMSE )\n",
    "# MAE\n",
    "xgb_MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE: %.3f\" %xgb_MAE )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de variables en testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTesting = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2023/main/datasets/dataTest_carListings.zip', index_col=0)\n",
    "dataTesting['Age'] = dataTesting['Year'].apply(lambda x : time_now.year - x)\n",
    "dataTesting = dataTesting.drop(['Year'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit transform\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataTraining[['Mileage']])\n",
    "dataTraining[['Mileage']] = scaler.transform(dataTraining[['Mileage']])\n",
    "dataTesting[['Mileage']] = scaler.transform(dataTesting[['Mileage']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Dummies marca ########################################\n",
    "dummies = pd.get_dummies(dataTesting['Make'], prefix='Make')\n",
    "dataTesting = pd.concat([dataTesting, dummies], axis=1)\n",
    "\n",
    "########################### Dummies modelo #######################################\n",
    "\n",
    "# Creamos las columnas con el nombre de la partición en el dataframe\n",
    "\n",
    "for i in range(len(column_names)):\n",
    "    dataTesting[column_names[i]]=0\n",
    "\n",
    "# Validar si el valor de la columna Model está en el diccionario rango_precios\n",
    "for i, row in dataTesting.iterrows():\n",
    "    model = row['Model']\n",
    "    for key, value in rango_precios.items():\n",
    "        if model in value:\n",
    "            dataTesting.at[i, key] = 1\n",
    "\n",
    "############################## Dummies para State ####################################\n",
    "dummies = pd.get_dummies(dataTesting['State'], prefix='State')\n",
    "dataTesting = pd.concat([dataTesting, dummies], axis=1)\n",
    "            \n",
    "dataTesting=dataTesting.drop(['State', 'Make','Model'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=MODELO.predict(dataTesting)\n",
    "y_pred = np.expm1(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar predicciones en formato exigido en la competencia de kaggle\n",
    "y_pred.to_csv('test_submission.csv', index_label='ID')\n",
    "y_pred.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
